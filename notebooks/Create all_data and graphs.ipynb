{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE ALL_DATA AND GRAPHS\n",
    "\n",
    "**Aim :** Create the all_data that are the DataFrame containing the information on one day, and the graph corresponding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "os.chdir(\"C:/Users/maell/Fake_News_Project\")\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import community\n",
    "\n",
    "from scipy import *\n",
    "\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_data(fichier):\n",
    "    data=pd.read_csv(fichier,sep=\"\\t\",index_col=0)\n",
    "    return data\n",
    "##############################################################################################################################\n",
    "\n",
    "def filtrer(src, dst, date):\n",
    "    date = [date]\n",
    "    for line in src:\n",
    "        if 'mentions.CSV' not in line:\n",
    "            continue\n",
    "        arr = line.split(' ')\n",
    "        for time in date:\n",
    "            if time in arr[2]:\n",
    "                dst.write(arr[0]+' '+arr[1]+' '+arr[2])\n",
    "\n",
    "##############################################################################################################################\n",
    "\n",
    "def dezip(filezip, pathdst = ''): \n",
    "    if pathdst == '': pathdst = os.getcwd()  ## on dezippe dans le repertoire locale \n",
    "    zfile = zipfile.ZipFile(filezip, 'r') \n",
    "    for i in zfile.namelist():  ## On parcourt l'ensemble des fichiers de l'archive \n",
    "        #print (i) \n",
    "        if os.path.isdir(i):   ## S'il s'agit d'un repertoire, on se contente de creer le dossier \n",
    "            try: os.makedirs(pathdst + os.sep + i) \n",
    "            except: pass \n",
    "        else: \n",
    "            try: os.makedirs(pathdst + os.sep + os.path.dirname(i)) \n",
    "            except: pass \n",
    "            data = zfile.read(i)                   ## lecture du fichier compresse \n",
    "            fp = open(pathdst + os.sep + i, \"wb\")  ## creation en local du nouveau fichier \n",
    "            fp.write(data)                         ## ajout des donnees du fichier compresse dans le fichier local \n",
    "            fp.close() \n",
    "    zfile.close()\n",
    "\n",
    "##############################################################################################################################\n",
    "\n",
    "def creation_fichier_csv(date):\n",
    "\n",
    "    # Création du fichier contenant uniquement les liens des fichier mentions qui nous interressent\n",
    "    source = open(\"masterfilelist.txt\", \"r\") # Ouverture du fichier source\n",
    "    destination = open(\"data_list.csv\", \"w\") # Ouverture du fichier destination\n",
    "    filtrer(source, destination, date) # Filtrage\n",
    "    destination.close()# Fermeture du fichier destination\n",
    "    source.close()# Fermerture du fichier source\n",
    "\n",
    "    #enregistrement fichier csv\n",
    "    master_data = pd.read_csv('data_list.csv',sep= ' ',header = None,engine='python')\n",
    "    master_data.columns = ['1','2','url']\n",
    "\n",
    "    #suppression lignes inutiles\n",
    "    for index, row in master_data.iterrows():\n",
    "        if '.mentions.CSV' not in row['url']:\n",
    "            master_data = master_data.drop(index)\n",
    "\n",
    "    # vide le dossier où stocker les donnees\n",
    "    for element in os.listdir(\"C:/Users/maell/Fake_News_Project/data_folder\"):\n",
    "        path=\"C:/Users/maell/Fake_News_Project/data_folder/\"+element\n",
    "        os.remove(path)\n",
    "\n",
    "    #telecharge et dezippe les fichiers depuis le web\n",
    "    for index, row in master_data.iterrows():\n",
    "        element = row['url']\n",
    "        urllib.request.urlretrieve(element,'file')\n",
    "        dezip('file', 'data_folder')\n",
    "    print(date+' dezip OK')\n",
    "\n",
    "    # Création du DataFrame Complet\n",
    "    column_names=['GlobalEventID','EventTimeDate','MentionTimeDate','MentionType','MentionSourceName','MentionIdentifier','SentenceID','Actor1CharOffset','Actor2CharOffset','ActionCharOffset','InRawText','Confidence','MentionDocLen','MentionDocTone','MentionDocTranslationInfo','Extras']\n",
    "    all_data=pd.DataFrame({'GlobalEventID':[],'EventTimeDate':[],'MentionTimeDate':[],'MentionType':[],'MentionSourceName':[],'MentionIdentifier':[],'SentenceID':[],'Actor1CharOffset':[],'Actor2CharOffset':[],'ActionCharOffset':[],'InRawText':[],'Confidence':[],'MentionDocLen':[],'MentionDocTone':[],'MentionDocTranslationInfo':[],'Extras':[]},columns = column_names, index=[])\n",
    "\n",
    "    for fichier in os.listdir(\"C:/Users/maell/Fake_News_Project/data_folder\"):\n",
    "        path_fichier=\"C:/Users/maell/Fake_News_Project/data_folder/\"+fichier\n",
    "        new_data=open_data(path_fichier)\n",
    "        all_data = pd.concat([all_data,new_data],ignore_index = True)\n",
    "\n",
    "    # Sauvegarde du fichier all_data.csv\n",
    "    nom_fichier='all_data/all_data_'+date+'.csv'\n",
    "    all_data.to_csv(nom_fichier, sep='\\t', decimal= '.')\n",
    "    print(nom_fichier+' enregistré')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20180623 dezip OK\n",
      "all_data/all_data_20180623.csv enregistré\n",
      "20180624 dezip OK\n",
      "all_data/all_data_20180624.csv enregistré\n",
      "20180625 dezip OK\n",
      "all_data/all_data_20180625.csv enregistré\n",
      "20180626 dezip OK\n",
      "all_data/all_data_20180626.csv enregistré\n",
      "20180627 dezip OK\n",
      "all_data/all_data_20180627.csv enregistré\n",
      "20180628 dezip OK\n",
      "all_data/all_data_20180628.csv enregistré\n",
      "20180629 dezip OK\n",
      "all_data/all_data_20180629.csv enregistré\n"
     ]
    }
   ],
   "source": [
    "list_date=['20180623', '20180624', '20180625', '20180626', '20180627', '20180628', '20180629']\n",
    "for date in list_date:\n",
    "    creation_fichier_csv(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE ALL DATA FOR 1 WEEK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataFrame(df_all_data):\n",
    "    all_data_reduit=df_all_data.drop_duplicates({'MentionTimeDate','GlobalEventID','MentionSourceName'})\n",
    "    df_all_data=all_data_reduit.dropna(subset=['MentionSourceName'])\n",
    "    return df_all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_date=['20170930','20170929']\n",
    "#['20180623', '20180624', '20180625', '20180626', '20180627', '20180628', '20180629','20180630']\n",
    "\n",
    "df_all_data=pd.DataFrame()\n",
    "df_republication=pd.DataFrame(columns=['WebsiteURL','Republication'])\n",
    "for date in list_date:\n",
    "    path='all_data/all_data_'+date+'.csv'\n",
    "    df_all_data=df_all_data.append(open_data(path))\n",
    "        \n",
    "df_all_data=clean_dataFrame(df_all_data)\n",
    "df_all_data.to_csv('2_days_all_data_20170929_20170930', sep='\\t', decimal= '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_sources_poids(all_data_path):\n",
    "    # Create graph where nodes are websites and edges reprensent the number of event shared between the two websites.\n",
    "    data=open_data(all_data_path)\n",
    "    G=nx.Graph()\n",
    "    for source in data.MentionSourceName.unique():\n",
    "        G.add_node(source)\n",
    "    for event in data.GlobalEventID.unique():\n",
    "        liste_source=data[data.GlobalEventID==event].MentionSourceName\n",
    "        for comb in combinations(liste_source,2):\n",
    "            if comb[0]!=comb[1]:\n",
    "                if G.has_edge(comb[0],comb[1]):\n",
    "                    G[comb[0]][comb[1]]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(comb[0], comb[1], weight=1)\n",
    "    return G\n",
    "\n",
    "\n",
    "     \n",
    "#######################################################################################################\n",
    "    \n",
    "def cleaning_graph(H):\n",
    "    G=nx.Graph.copy(H)\n",
    "    \n",
    "    # Suppression des nan\n",
    "    list_source=list(G.nodes)\n",
    "    for source in list_source:\n",
    "        if type(source) is not str:\n",
    "            G.remove_node(source)\n",
    "    \n",
    "    # Remove edges not weigted enough\n",
    "    edges_to_remove = []\n",
    "    for edge in G.edges():\n",
    "        if G[edge[0]][edge[1]]['weight']<5:\n",
    "            edges_to_remove.append(edge)\n",
    "    if(edges_to_remove!=[]):\n",
    "        G.remove_edges_from(edges_to_remove)\n",
    "\n",
    "    # Suppression des éléments non connectés\n",
    "    composants=list(nx.connected_components(G))\n",
    "    i=0;\n",
    "    while i<len(composants):\n",
    "        if len(composants[i])==1:\n",
    "            composant_supp=list(composants[i])[0]\n",
    "            G.remove_node(composant_supp)\n",
    "        i=i+1     \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to clean\n",
      "Graph Created\n"
     ]
    }
   ],
   "source": [
    "all_data='2_days_all_data_20170929_20170930'\n",
    "G=graph_sources_poids(all_data)\n",
    "print('Ready to clean')\n",
    "G_clean=cleaning_graph(G);\n",
    "nom_graph='graph_20170929_20170930.gexf'\n",
    "nx.write_gexf(G_clean,nom_graph)\n",
    "print('Graph Created')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
