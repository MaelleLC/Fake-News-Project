{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "os.chdir('C:\\\\...')\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "import glob\n",
    "\n",
    "import math\n",
    "from itertools import combinations\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random as rd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import *\n",
    "import matplotlib.animation\n",
    "\n",
    "import networkx as nx\n",
    "import community\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "from pytrends.request import TrendReq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ouverture des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'master_data_en.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2c2b269615ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Ouverture du fichier source\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0msource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"master_data_en.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Ouverture du fichier destination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'master_data_en.txt'"
     ]
    }
   ],
   "source": [
    "# write on data.csv the name of datas to save on the computer\n",
    "\n",
    "def filtrer(src, dst):\n",
    "    date = ['2017120','20171210','20171211']\n",
    "    \n",
    "    for line in src:\n",
    "        if 'mentions.CSV' not in line:\n",
    "            continue\n",
    "        arr = line.split(' ')\n",
    "        for time in date:\n",
    "            if time in arr[2]:\n",
    "                dst.write(arr[0]+' '+arr[1]+' '+arr[2])\n",
    "    \n",
    "# Ouverture du fichier source\n",
    "source = open(\"master_data_en.txt\", \"r\")\n",
    " \n",
    "# Ouverture du fichier destination\n",
    "destination = open(\"data.csv\", \"w\")\n",
    " \n",
    "# Appeler la fonction de traitement\n",
    "filtrer(source, destination)\n",
    " \n",
    "# Fermeture du fichier destination\n",
    "destination.close()\n",
    " \n",
    "# Fermerture du fichier source\n",
    "source.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b304cf272ea9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m#enregistrement fichier csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mmaster_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'python'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mmaster_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'url'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1022\u001b[0m                                  ' \"c\", \"python\", or' ' \"python-fwf\")'.format(\n\u001b[0;32m   1023\u001b[0m                                      engine=engine))\n\u001b[1;32m-> 1024\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1026\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, **kwds)\u001b[0m\n\u001b[0;32m   2075\u001b[0m         f, handles = _get_handle(f, mode, encoding=self.encoding,\n\u001b[0;32m   2076\u001b[0m                                  \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2077\u001b[1;33m                                  memory_map=self.memory_map)\n\u001b[0m\u001b[0;32m   2078\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2079\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    401\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[1;31m# Python 3 and no explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'replace'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    404\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m             \u001b[1;31m# Python 3 and binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.csv'"
     ]
    }
   ],
   "source": [
    "# download and dezip files from data.csv\n",
    "\n",
    "def dezip(filezip, pathdst = ''): \n",
    "    if pathdst == '': pathdst = os.getcwd()  ## on dezippe dans le repertoire locale \n",
    "    zfile = zipfile.ZipFile(filezip, 'r') \n",
    "    for i in zfile.namelist():  ## On parcourt l'ensemble des fichiers de l'archive \n",
    "        print (i) \n",
    "        if os.path.isdir(i):   ## S'il s'agit d'un repertoire, on se contente de creer le dossier \n",
    "            try: os.makedirs(pathdst + os.sep + i) \n",
    "            except: pass \n",
    "        else: \n",
    "            try: os.makedirs(pathdst + os.sep + os.path.dirname(i)) \n",
    "            except: pass \n",
    "            data = zfile.read(i)                   ## lecture du fichier compresse \n",
    "            fp = open(pathdst + os.sep + i, \"wb\")  ## creation en local du nouveau fichier \n",
    "            fp.write(data)                         ## ajout des donnees du fichier compresse dans le fichier local \n",
    "            fp.close() \n",
    "    zfile.close() \n",
    "\n",
    "#enregistrement fichier csv\n",
    "master_data = pd.read_csv('data.csv',sep= ' ',header = None,engine='python')\n",
    "master_data.columns = ['1','2','url']\n",
    "\n",
    "#suppression lignes inutiles\n",
    "for index, row in master_data.iterrows():\n",
    "    if '.mentions.CSV' not in row['url']:\n",
    "        master_data = master_data.drop(index)\n",
    "\n",
    "# vide le dossier où stocker les donnees\n",
    "for element in os.listdir('C:\\\\...'):\n",
    "    os.remove(element)\n",
    "        \n",
    "#telecharge et dezippe les fichiers depuis le web\n",
    "for index, row in master_data.iterrows():\n",
    "    element = row['url']\n",
    "    urllib.request.urlretrieve(element,'file')\n",
    "    dezip('file', 'data_folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create the dataframe from all files of the GDELT dataset\n",
    "\n",
    "# # all datas have to be in the same folder\n",
    "# all_data= pd.DataFrame({'ID': [], 'EventDate': [], 'MentionDate' : [], 'Source': [], \n",
    "#                         'Article': [], 'Sentence': []}, \n",
    "#                        columns = ['ID', 'EventDate', 'MentionDate', 'Source', \n",
    "#                                   'Article', 'Sentence'])\n",
    "\n",
    "# # choose the number of files to use\n",
    "# # one file per quarter hour : one week = 692\n",
    "# max_iter = 692\n",
    "# compte = 0\n",
    "\n",
    "# for element in os.listdir('C:\\\\...):\n",
    "#     if '.CSV' in element:\n",
    "#         if compte < max_iter:\n",
    "#         #enregistrement fichier csv\n",
    "#             new_data = pd.read_csv('wikileaks\\\\'+element,parse_dates=[1,2],sep= '\\t', decimal= '.',header = None)\n",
    "#             #suppression colonnes inutiles\n",
    "#             for c in new_data.columns:\n",
    "#                 if c not in [0,1,2,4,5,6]:\n",
    "#                     del new_data[c]\n",
    "#             new_data.columns = ['ID','EventDate','MentionDate','Source','Article','Sentence']\n",
    "#             all_data = pd.concat([all_data,new_data])\n",
    "#             print(element+' : done')\n",
    "#             compte +=1\n",
    "# print(\"completed\")\n",
    "\n",
    "# # save the dataframe into a csv file to use it next time\n",
    "\n",
    "# all_data.to_csv('all_data.csv', sep= '\\t', decimal= '.')\n",
    "\n",
    "# # read the csv file and put it in a dataframe\n",
    "# all_data = pd.read_csv('all_data.csv',parse_dates=[1,2],sep = '\\t',decimal = '.')\n",
    "# for c in all_data.columns:\n",
    "#     if c not in  ['ID', 'EventDate', 'MentionDate', 'Source', 'Article', 'Sentence']:\n",
    "#         del all_data[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of events, articles and mention_date of the dataset\n",
    "print('#events :', len(all_data.ID.unique()), '#sources :', len(all_data.Source.unique()), '#articles', len(all_data.Article.unique()), '#mention_date', len(all_data.MentionDate.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source graphe\n",
    "# nodes : sources / edges : 2 sources shares the same event in the same mention date\n",
    "# edges weight : number of shared events\n",
    "# /!\\ some sources are not represented in the graphe if they are not linked to a least one source\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for date in all_data.MentionDate.unique():\n",
    "    data_t = all_data[all_data['MentionDate'].isin([date])]\n",
    "    for event in data_t.ID.unique():\n",
    "        data_e = data_t[data_t['ID'].isin([event])]\n",
    "        for comb in combinations(data_e['Source'],2):\n",
    "            if comb[0]!=comb[1]:\n",
    "                if G.has_edge(comb[0],comb[1]):\n",
    "                    G[comb[0]][comb[1]]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(comb[0], comb[1], weight=1)\n",
    "                    \n",
    "nx.write_gexf(G,'time_source_1.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source graphe\n",
    "# nodes : sources / edges : 2 sources shares the same event\n",
    "# edges weight : number of shared events\n",
    "# /!\\ take to much time for 1 week dataset\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for event in all_data.ID.unique():\n",
    "    data_e = all_data[all_data['ID'].isin([event])]\n",
    "    for comb in combinations(data_e['Source'],2):\n",
    "        if comb[0]!=comb[1]:\n",
    "            if G.has_edge(comb[0],comb[1]):\n",
    "                G[comb[0]][comb[1]]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(comb[0], comb[1], weight=1)\n",
    "nx.write_gexf(G,'time_source_2.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove edges with a weight < 100\n",
    "\n",
    "edges_to_remove = []\n",
    "\n",
    "H = G.copy()\n",
    "\n",
    "for edge in H.edges():\n",
    "    if H[edge[0]][edge[1]]['weight']<100:\n",
    "        edges_to_remove.append(edge)\n",
    "\n",
    "H.remove_edges_from(edges_to_remove)\n",
    "\n",
    "nx.write_gexf(H,'connected_source_filtered.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphe des sources à partir du fichier graphml\n",
    "\n",
    "# lit le fichier graphml comportant le graphe souhaité\n",
    "H = nx.read_graphml(path='sources.graphml')\n",
    "\n",
    "# détection de communauté\n",
    "partition = community.best_partition(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positions des sources à partir de fichier graphml de gephi\n",
    "pos = {}\n",
    "\n",
    "# Ouverture du fichier source \n",
    "source = open(\"sources.graphml\", \"r\")\n",
    "\n",
    "# Appeler la fonction de traitement\n",
    "for line in source:\n",
    "    if '<node id' in line:\n",
    "        line_i = line.replace('<node id=\"',\"\")\n",
    "        node = line_i.replace('\">\\n',\"\")\n",
    "    if '<data key=\"x\"' in line:\n",
    "        line_i = line.replace('<data key=\"x\">',\"\")\n",
    "        x = line_i.replace('</data>\\n',\"\")\n",
    "        pos[node] = [float(x)]\n",
    "    if '<data key=\"y\"' in line:\n",
    "        line_i = line.replace('<data key=\"y\">',\"\")\n",
    "        y = line_i.replace('</data>\\n',\"\")\n",
    "        pos[node].append(float(y))\n",
    "\n",
    "# Fermerture du fichier source\n",
    "source.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction de Lissage d'une courbe par moyenne glissante. Le paramètre p determine la qualité du lissage\n",
    "def lissage(Lx,Ly,p):\n",
    "    Lxout=[]\n",
    "    Lyout=[]\n",
    "    Lxout = Lx[p: -p]\n",
    "    for index in range(p, len(Ly)-p):\n",
    "        average = sum(Ly[index-p : index+p+1]) / (2*p + 1)\n",
    "        Lyout.append(average)\n",
    "    return Lxout,Lyout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_dict = {source : #articles}\n",
    "# /!\\ 45 MINUTES D'ATTENTE /!\\\n",
    "article_dict = {}\n",
    "for src in all_data.Source.unique():\n",
    "    article_dict[src] = len(all_data[all_data['Source'].isin([src])].Article.unique())\n",
    "    \n",
    "article_dict_sorted = sorted(article_dict.items(), key=lambda t: t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the 10 most recurrent sources of the dataset\n",
    "\n",
    "rec_sources = article_dict_sorted[-11:]\n",
    "\n",
    "plt.style.use(['fivethirtyeight','seaborn-white'])\n",
    "\n",
    "source = []\n",
    "frequency = []\n",
    "\n",
    "for i in range(len(rec_sources)):\n",
    "    source.append(rec_sources[i][0])\n",
    "    frequency.append(rec_sources[i][1])\n",
    "\n",
    "plt.figure(1)\n",
    "plt.bar(source, frequency)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylabel('#Articles')\n",
    "plt.title('Most recurrent sources')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('most_recurrent_sources.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the five heaviest links of the graphe\n",
    "\n",
    "edge_list = [('wacotrib.com\\nchieftain.com', 5515.0), \n",
    "            ('wacotrib.com\\ngreensboro.com', 5310.0),\n",
    "            ('greensboro.com\\nroanoke.com', 4906.0), \n",
    "            ('wacotrib.com\\nroanoke.com', 4706.0), \n",
    "            ('greensboro.com\\nchieftain.com', 4705.0)]\n",
    "\n",
    "fig = figure(2, figsize=(10, 6))\n",
    "plt.style.use(['fivethirtyeight','seaborn-white'])\n",
    "\n",
    "source = []\n",
    "frequency = []\n",
    "\n",
    "for i in range(len(edge_list)):\n",
    "    source.append(edge_list[i][0])\n",
    "    frequency.append(edge_list[i][1])\n",
    "\n",
    "plt.figure(2)\n",
    "plt.bar(source, frequency)\n",
    "plt.ylabel('Weight')\n",
    "plt.title('Most connected sources')\n",
    "plt.legend()\n",
    "plt.savefig('most_connected_sources.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taux de republication pour les 5 sources les plus influentes du web\n",
    "# calculé sur 500 events pris aléatoirement pour chaque source\n",
    "\n",
    "rep_rate = {}\n",
    "pop_source = ['yahoo.com','cnn.com','nytimes.com','msn.com','breitbart.com']\n",
    "for source in pop_source:\n",
    "    rep_rate[source] = [100,0]\n",
    "    data_source = all_data[all_data['Source'].isin([source])]\n",
    "    compteur = 0\n",
    "    event_liste = []\n",
    "    events = data_source.ID.unique()\n",
    "\n",
    "    while compteur < 500:\n",
    "        event = random.choice(events)\n",
    "        event_liste.append(event)\n",
    "        compteur+=1\n",
    "            \n",
    "    for event in  event_liste:\n",
    "        source_mention = data_source[data_source['ID'].isin([event])]['MentionDate'].iloc[0]\n",
    "        first_mention = all_data[all_data['ID'].isin([event])]['MentionDate'].iloc[0]\n",
    "        if source_mention == first_mention:\n",
    "            rep_rate[source][1] += 1\n",
    "\n",
    "rate = [1-rep_rate[k][1]/rep_rate[k][0] for k in rep_rate.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Trends by Volodymyr Miz\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "keywords = ['wikileaks']\n",
    "\n",
    "pytrend = TrendReq(hl='en-US')\n",
    "pytrend.build_payload(kw_list=keywords, timeframe='2017-12-01 2017-12-20', geo='', gprop='')\n",
    "\n",
    "interest_over_time_df = pytrend.interest_over_time()\n",
    "interest_over_time_df.shape\n",
    "interest_over_time_df.plot()\n",
    "plt.savefig('google_trends.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heavy = {}\n",
    "for n in [node for node in partition.keys() if partition[node]==4]:\n",
    "    neigbors = {k : H[n][k]['weight'] for k in H[n].keys()}\n",
    "    neigbors_sorted = sorted(neigbors.items(), key=lambda t: t[1])\n",
    "    n_heavy[n] = neigbors_sorted[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heaviest connections in the graph\n",
    "for edge in H.edges():\n",
    "    if H[edge[0]][edge[1]]['weight'] > 3000:\n",
    "        print(edge,partition[edge[0]],partition[edge[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# informations sur les liaisons\n",
    "\n",
    "heav_com = {}\n",
    "for com in range(0,11):\n",
    "#     max_heav = 0.\n",
    "#     for n in [node for node in partition.keys() if partition[node]==com]:\n",
    "#         neigbors = {k : H[n][k]['weight'] for k in H[n].keys()}\n",
    "#         neigbors_sorted = sorted(neigbors.items(), key=lambda t: t[1])\n",
    "#         if neigbors_sorted[-1][1] > max_heav:\n",
    "#             max_heav = neigbors_sorted[-1][1]\n",
    "#     heav_com[com] = max_heav\n",
    "    w = 0\n",
    "    tot = 0\n",
    "    for n in [node for node in partition.keys() if partition[node]==com]:\n",
    "        for k in H.neighbors(n):\n",
    "            if partition[k] == com:\n",
    "                w += H[n][k]['weight']\n",
    "                tot += 1\n",
    "    heav_com[com] = w/tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# génère une palette de n couleurs aléatoires\n",
    "\n",
    "def RGB_to_hex(RGB):\n",
    "#   [255,255,255] -> \"#FFFFFF\"\n",
    "    # Components need to be integers for hex to make sense\n",
    "    RGB = [int(x) for x in RGB]\n",
    "    return \"#\"+\"\".join([\"0{0:x}\".format(v) if v < 16 else\n",
    "            \"{0:x}\".format(v) for v in RGB])\n",
    "\n",
    "def rand_hex_color(num=1):\n",
    "#     Generate random hex colors, default is one, returning a string. If num is greater than\n",
    "#     1, an array of strings is returned.\n",
    "    colors = [\n",
    "    RGB_to_hex([x*255 for x in rnd.rand(3)])\n",
    "    for i in range(num)\n",
    "    ]\n",
    "    if num == 1:\n",
    "        return colors[0]\n",
    "    else:\n",
    "        return colors\n",
    "\n",
    "n = len(list(set(list(partition.values()))))\n",
    "\n",
    "colors_list = rand_hex_color(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "n_col = []\n",
    "\n",
    "fig = figure(1, figsize=(10, 10))\n",
    "\n",
    "x_min =-3000\n",
    "x_max =3000\n",
    "y_min =-3000\n",
    "y_max =3000\n",
    "\n",
    "# x_min = 450\n",
    "# x_max = 550\n",
    "# y_min = 450\n",
    "# y_max = 550\n",
    "\n",
    "for node in pos.keys():\n",
    "    if pos[node][0]>x_min and pos[node][0]<x_max and pos[node][1]>y_min and pos[node][1]<y_max:\n",
    "        x.append(pos[node][0])\n",
    "        y.append(pos[node][1])\n",
    "        col = colors_list[partition[node]]\n",
    "        n_col.append(col)\n",
    "plt.figure(1)\n",
    "plt.scatter(x, y, s=1, color = n_col)\n",
    "plt.title('Linked source')\n",
    "plt.savefig('linked_source_communities_1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affiche les communautés présent dans le cadre [(x_min, x_max), (y_min, y_max)]\n",
    "\n",
    "x_min = 450\n",
    "x_max = 550\n",
    "y_min = 450\n",
    "y_max = 550\n",
    "\n",
    "for node in pos.keys():\n",
    "    if pos[node][0]>x_min and pos[node][0]<x_max and pos[node][1]>y_min and pos[node][1]<y_max:\n",
    "        print(partition[node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affiche les sources appartenant à la communauté n et ayant écrit plus de 100 articles sur la période\n",
    "# affiche quelques statistiques sur la comnunauté n\n",
    "\n",
    "communauté = [node for node in partition.keys() if partition[node] == n]\n",
    "art = []\n",
    "n =\n",
    "\n",
    "for source in communauté:\n",
    "    if len(all_data[all_data['Source'].isin([source])].Article.unique())>100:\n",
    "        print(source)\n",
    "\n",
    "for source in communauté:\n",
    "    art.append(len(all_data[all_data['Source'].isin([source])].Article.unique()))\n",
    "    \n",
    "art_np = np.array(art)\n",
    "\n",
    "art_m = np.mean(art_np, 0)\n",
    "art_std = np.std(art_np, 0, ddof=1)\n",
    "art_M = np.median(art_np, 0)\n",
    "print('mean : %f, std : %f, med : %f, tot : %d' %(art_m,art_std,art_M,len(communauté)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_pos_df= pd.DataFrame({'Position_x': x, 'Position_y': y}, \n",
    "                            columns = ['Position_x', 'Position_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom the color\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "\n",
    "# Custom the inside plot: options are: “scatter” | “reg” | “resid” | “kde” | “hex”\n",
    "sns_plot = sns.jointplot(x = source_pos_df['Position_x'], y = source_pos_df['Position_y'], kind='kde', color=\"skyblue\")\n",
    "sns_plot.savefig('kde_source.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compteur=0\n",
    "for index,row in data_event.iterrows():\n",
    "    if compteur == 10:\n",
    "        continue\n",
    "    print(row['Article'],row['Sentence'])\n",
    "    compteur+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_on = {}\n",
    "for key in dict_mention.keys():\n",
    "    if key in pos:\n",
    "        if dict_mention[key] in source_on:\n",
    "            source_on[dict_mention[key]].append(pos[key])\n",
    "        else:\n",
    "            source_on[dict_mention[key]] = [pos[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = int(max(key for key in source_on.keys()))\n",
    "print(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for event in event_liste:\n",
    "    data_event = all_data[all_data['ID'].isin([event])]\n",
    "    quantity = len(data_event)\n",
    "    ti = data_event.iloc[0]['MentionDate']\n",
    "    tf = data_event.iloc[-1]['MentionDate']\n",
    "    ti_dt = datetime.strptime(ti, '%Y-%m-%d %X')\n",
    "    tf_dt = datetime.strptime(tf, '%Y-%m-%d %X')\n",
    "    delta_t = (tf_dt - ti_dt).seconds\n",
    "    delta_t /= 60\n",
    "    x.append(int(delta_t))\n",
    "    y.append(quantity)\n",
    "    \n",
    "time_qty_df= pd.DataFrame({'Qty': y, 'Time': x}, columns = ['Qty', 'Time'])\n",
    "\n",
    "# Custom the color\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "\n",
    "# Custom the inside plot\n",
    "sns_plot = sns.jointplot(x = time_qty_df['Time'], y = time_qty_df['Qty'], kind='kde', color=\"skyblue\")\n",
    "sns_plot.savefig('time_vs_qty_kde.png')\n",
    "\n",
    "time_qty_df_z = time_qty_df.query('Qty < 150')\n",
    "\n",
    "# Custom the inside plot\n",
    "sns_plot = sns.jointplot(x = time_qty_df_z['Time'], y = time_qty_df_z['Qty'], kind='kde', color=\"skyblue\")\n",
    "sns_plot.savefig('time_vs_qty_z_kde.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom sur le noyau central\n",
    "\n",
    "x_z = []\n",
    "y_z = []\n",
    "pos_z = {}\n",
    "        \n",
    "for source in pos.keys():\n",
    "    if abs(pos[source][0]) < 3000 and abs(pos[source][1]) < 3000:\n",
    "        x_z.append(pos[source][0])\n",
    "        y_z.append(pos[source][1])\n",
    "        pos_z[source] = pos[source]\n",
    "        \n",
    "plt.plot(x_z,y_z,ms=0.3,color='k',marker='o',ls='')\n",
    "\n",
    "plt.title('Linked source')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionnaire dict_mention = {source : delta_t}\n",
    "\n",
    "dict_mention = {}\n",
    "\n",
    "# data_event = all_data[all_data['ID'].isin([713561281.0])]\n",
    "data_event = all_data[all_data['ID'].isin([714037633.0])]\n",
    "\n",
    "t0 = data_event.iloc[0]['MentionDate']\n",
    "for index, row in data_event.iterrows():\n",
    "    if row['Source'] in dict_mention:\n",
    "        continue \n",
    "    t = row['MentionDate']\n",
    "    delta_t = (t - t0).seconds\n",
    "    delta_t /= 60\n",
    "    dict_mention[row['Source']] = int(delta_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event source dictionnaire \n",
    "\n",
    "source_on = {}\n",
    "for key in dict_mention.keys():\n",
    "    if key in pos_z:\n",
    "        if dict_mention[key] in source_on:\n",
    "            source_on[dict_mention[key]].append(pos[key])\n",
    "        else:\n",
    "            source_on[dict_mention[key]] = [pos[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = int(max(key for key in source_on.keys())/15)\n",
    "print(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animate the propagation of an event\n",
    "\n",
    "os.system('ffmpeg')\n",
    "\n",
    "# First set up the figure, the axis, and the plot element we want to animate\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(xlim=(-3000, 3000), ylim=(-3000, 3000))\n",
    "scat, = ax.plot([], [], ms=0.2,color='k',marker='o',ls='')\n",
    "new_scat, = ax.plot([], [], ms=1,color='red',marker='o',ls='')\n",
    "u = []\n",
    "v = []\n",
    "\n",
    "# initialization function: plot the background of each frame\n",
    "def init():\n",
    "    for p in pos_z.values():\n",
    "        x.append(p[0])\n",
    "        y.append(p[1])\n",
    "    scat.set_data(x, y)\n",
    "    return scat,\n",
    "\n",
    "# animation function.  This is called sequentially\n",
    "def animate(time):\n",
    "    if time*5 in source_on:\n",
    "        for point in source_on[time*5]:\n",
    "            u.append(point[0])\n",
    "            v.append(point[1])\n",
    "    new_scat.set_data(u, v)\n",
    "    return new_scat,\n",
    "\n",
    "# call the animator.  blit=True means only re-draw the parts that have changed.\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=frames, blit=True)\n",
    "\n",
    "# save the animation as an mp4 where 1 second is equivalent to 1 hour\n",
    "anim.save('714037633_real.mp4', fps=12, extra_args=['-vcodec', 'libx264'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom sur le noyau central\n",
    "\n",
    "x_z = []\n",
    "y_z = []\n",
    "pos_z = {}\n",
    "        \n",
    "for source in pos.keys():\n",
    "    if pos[source][0] < -500 and pos[source][0] > -1250 and pos[source][1] < 500 and pos[source][1] > 0:\n",
    "#     if pos[source][0] < 0 and pos[source][0] > -2000 and pos[source][1] < 1000 and pos[source][1] > -1000:\n",
    "        x_z.append(pos[source][0])\n",
    "        y_z.append(pos[source][1])\n",
    "        pos_z[source] = pos[source]\n",
    "        print(source)\n",
    "        \n",
    "plt.plot(x_z,y_z,ms=2,color='k',marker='o',ls='')\n",
    "\n",
    "plt.title('Linked source')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.savefig('wiki_source_z.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_list = all_data.ID.unique()\n",
    "compteur = 0\n",
    "event = random.choice(event_list)\n",
    "\n",
    "while len(all_data[all_data['ID'].isin([event])]) < 130 or compteur < 3:\n",
    "    event = random.choice(event_list)\n",
    "    if len(all_data[all_data['ID'].isin([event])]) >= 130:\n",
    "        compteur +=1\n",
    "        print(compteur, event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction de Lissage d'une courbe par moyenne glissante. Le paramètre p determine la qualité du lissage\n",
    "def lissage(Lx,Ly,p):\n",
    "    Lxout=[]\n",
    "    Lyout=[]\n",
    "    Lxout = Lx[p: -p]\n",
    "    for index in range(p, len(Ly)-p):\n",
    "        average = sum(Ly[index-p : index+p+1]) / (2*p + 1)\n",
    "        Lyout.append(average)\n",
    "    return Lxout,Lyout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to find fake news?\n",
    "\n",
    "# fake_news return sorted event dictionnary mentionned by news_src with the number of mentions in other sources\n",
    "def news(news_src):\n",
    "    c=0 # impose une limite si il y a trop d'event pour une source donnée\n",
    "    news_dict = {}\n",
    "    news_data = all_data[all_data['Source'].isin([news_src])]\n",
    "    for id in news_data.ID.unique():\n",
    "        if all_data[all_data['ID'].isin([id])]['MentionDate'].iloc[0] == news_data[news_data['ID'].isin([id])]['MentionDate'].iloc[0]:\n",
    "            news_dict[id] = len(all_data[all_data['ID'].isin([id])])\n",
    "        if c == 1000:\n",
    "            break\n",
    "        c+=1\n",
    "\n",
    "    news_dict_sorted = sorted(news_dict.items(), key=lambda t: t[1])\n",
    "    \n",
    "    return(news_dict_sorted, news_dict)\n",
    "\n",
    "# checking sources bias : mediabiasfactcheck.com\n",
    "# interesting sources : snopes.com, infowars.com, theonion.com, breitbart.com\n",
    "# InfoWars is a far-right American conspiracy theorist and fake news website and media platform\n",
    "# The Onion is an American digital media company and news satire organization that publishes articles on \n",
    "# international, national, and local news : not enough propagation\n",
    "# Breitbart News Network is a far-righ syndicated American news, opinion and commentary website\n",
    "\n",
    "source = 'theonion.com'\n",
    "D_s, D = news(source)\n",
    "\n",
    "D_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find 3 events created by a source which produce more than 100 articles\n",
    "\n",
    "data_source = all_data[all_data['Source'].isin([source])]\n",
    "compteur = 0\n",
    "event_liste = []\n",
    "events = [k for k in D]\n",
    "\n",
    "# while compteur < 3:\n",
    "for event in events:\n",
    "#         event = random.choice(events)\n",
    "    source_mention = data_source[data_source['ID'].isin([event])]['MentionDate'].iloc[0]\n",
    "    first_mention = all_data[all_data['ID'].isin([event])]['MentionDate'].iloc[0]\n",
    "    if source_mention == first_mention and D[event]>5:\n",
    "        event_liste.append(event)\n",
    "        compteur+=1\n",
    "        \n",
    "print(event_liste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the 10 events by the source which produce the highest articles number\n",
    "for id in D_s[-10:]:\n",
    "    for index, row in all_data[all_data['ID'].isin([id[0]])].iterrows():\n",
    "        if row['Source'] == source:\n",
    "            print(id, row['Article'],row['Sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some informations about one event\n",
    "compteur = 0\n",
    "event = '712374053.0'\n",
    "for index,row in all_data[all_data['ID'].isin([event])].iterrows():\n",
    "    print(row['Article'],row['Sentence'])\n",
    "    compteur += 1\n",
    "    if compteur == 30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# somes news created by some interesting sources\n",
    "\n",
    "# https://www.infowars.com/u-s-moves-to-criminalize-anonymous-cryptocurrency-ownership/\n",
    "# https://www.infowars.com/cnns-acosta-when-journalists-attacked-they-must-resist/\n",
    "infowars_id = ['712370880.0','712369974.0']\n",
    "\n",
    "# http://www.breitbart.com/jerusalem/2017/12/05/hallelujah-president-donald-trump-will-recognize-jerusalem-as-capitol-of-israel-begin-moving-embassy/ 16.0\n",
    "# http://www.breitbart.com/big-government/2017/12/05/do-not-talk-to-me-about-honor-and-integrity-steve-bannon-savages-entire-romney-clan-after-mitts-insult-to-vietnam-vet-roy-moore/ 16.0\n",
    "breitbart_id = ['712726389.0','712766837.0']\n",
    "\n",
    "# https://www.reuters.com/article/us-honduras-election/honduran-candidate-calls-on-army-to-rebel-amid-disputed-vote-count-idUSKBN1DX0X7 6.0\n",
    "# https://www.reuters.com/article/us-northkorea-missiles-drill/south-korea-u-s-kick-off-largest-air-exercise-amid-north-korean-warnings-idUSKBN1DY02I 11.0\n",
    "reuters_id = ['712080245.0', '712090711.0']\n",
    "\n",
    "# /!\\ not enough propagation in other sources : only 14 articles\n",
    "# https://www.buzzfeed.com/hayesbrown/perry-of-arabia\n",
    "buzzfeed_id = ['712726898.0']\n",
    "\n",
    "# /!\\ not enough propagation in other sources : only 6 articles\n",
    "# https://politics.theonion.com/melania-trump-hangs-decayed-badger-carcass-over-white-h-1820886857\n",
    "theonion_id = ['712374053.0']\n",
    "\n",
    "# http://www.zerohedge.com/news/2017-12-05/erdogan-tells-trump-declaring-jerusalem-capital-israel-would-be-red-line 15.0\n",
    "# http://www.zerohedge.com/news/2017-12-03/legal-system-failing-america-when-it-comes-immigration 10.0\n",
    "zerohedge_id = ['712525070.0','712111700.0']\n",
    "\n",
    "event_dict = {'breitbart' : breitbart_id, 'infowars' : infowars_id, 'reuters' : reuters_id,\n",
    "              'buzzfeed' : buzzfeed_id, 'zerohedge' : zerohedge_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste de dictionnaires dict_mention_list = [{source : delta_t}]\n",
    "# liste de dictionnaires dict_delta_t_list = [{delta_t : nombre d'article}]\n",
    "# dictionnaire des events dict_event = [{event : url}]\n",
    "\n",
    "# 4 colors\n",
    "four_colors = ['#EC4A94','#F3A530','#88C542','#367ABD']\n",
    "\n",
    "# 5 colors\n",
    "five_colors = ['#EE4035','#F0A32F','#4CB2D4','#844D9E','#56B949']\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "fig = figure(1, figsize=(24, 8))\n",
    "\n",
    "i = 0\n",
    "\n",
    "# event_dict contient 10 real events et 10 fake events\n",
    "for k in event_dict:\n",
    "# calcule le nombre d'article par période de temps pour 10 events\n",
    "    dict_mention_list = []\n",
    "    dict_delta_t_list = []\n",
    "    dict_delta_t_cumul_list = []\n",
    "    dict_event = []\n",
    "    delta = 15.0\n",
    "\n",
    "    for event in event_dict[k]:\n",
    "        D={}\n",
    "        E={}\n",
    "        F={}\n",
    "        data_event = all_data[all_data['ID'].isin([str(event)])]\n",
    "        t0 = data_event.iloc[0]['MentionDate']\n",
    "        t0_dt = datetime.strptime(t0, '%Y-%m-%d %X')\n",
    "        for index, row in data_event.iterrows():\n",
    "            t = row['MentionDate']\n",
    "            t_dt = datetime.strptime(t, '%Y-%m-%d %X')\n",
    "            delta_t = (t_dt - t0_dt).seconds\n",
    "            delta_t /= 60           \n",
    "            D[row['Source']] = delta_t\n",
    "            if delta_t in E:\n",
    "                E[delta_t]+=1\n",
    "            else:\n",
    "                E[delta_t]=1\n",
    "            if delta_t in F:\n",
    "                F[delta_t]+=1\n",
    "            elif F:\n",
    "                F[delta_t] = max(F[d] for d in F)+1\n",
    "            else:\n",
    "                F[delta_t]=1\n",
    "        dict_mention_list.append(D)\n",
    "        dict_delta_t_list.append(E)\n",
    "        dict_delta_t_cumul_list.append(F)\n",
    "        dict_event.append({event : data_event.iloc[0]['Article']})\n",
    "    \n",
    "# graphe du #new articles\n",
    "    j = 1\n",
    "    for D in dict_delta_t_list:\n",
    "        lists = sorted(D.items()) # sorted by key, return a list of tuples\n",
    "        t, qty = zip(*lists) # unpack a list of pairs into two tuples\n",
    "        plt.subplot(121)  \n",
    "        if j == len(event_dict[k]):    \n",
    "            plt.plot(t, qty,label = k,color=five_colors[i])\n",
    "        else:\n",
    "            plt.plot(t, qty, color=five_colors[i])\n",
    "        j+=1\n",
    "    plt.xlabel('Time[min]')\n",
    "    plt.ylabel('#New articles')\n",
    "    plt.title(\"#New articles vs Time\")\n",
    "    plt.legend()\n",
    "\n",
    "# graphe du #articles\n",
    "    j = 1\n",
    "    for D in dict_delta_t_cumul_list:\n",
    "        lists = sorted(D.items()) # sorted by key, return a list of tuples\n",
    "        t, qty = zip(*lists) # unpack a list of pairs into two tuples\n",
    "        for key in dict_event[dict_delta_t_cumul_list.index(D)]:\n",
    "            l = key\n",
    "        plt.subplot(122)  \n",
    "        if j == len(event_dict[k]):    \n",
    "            plt.plot(t,qty,label = k,color=five_colors[i])\n",
    "        else:\n",
    "            plt.plot(t,qty,color=five_colors[i])\n",
    "        j+=1\n",
    "    plt.xlabel('Times[min]')\n",
    "    plt.ylabel('#Articles')\n",
    "    plt.title(\"#Articles vs Time\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig('news_across_the_time.png')\n",
    "    \n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
